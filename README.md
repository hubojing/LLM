# LLM
大模型自用资料。

# 前沿解读
- https://magazine.sebastianraschka.com/

# GGUF模型下载
- https://modelscope.cn/models

# 论文 & 模型 & 技术报告
## deepseek
- DeepSeek-VL2: https://github.com/deepseek-ai/DeepSeek-VL2/blob/main/DeepSeek_VL2_paper.pdf  https://arxiv.org/pdf/2412.10302  
- DeepSeek-R1: Incentivizing Reasoning Capability in LLMs viaReinforcement Learning  https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf  
- DeepSeek-V3 Technical Report  https://github.com/deepseek-ai/DeepSeek-V3/blob/main/DeepSeek_V3.pdf  
- DeepSeek-Coder: When the Large Language Model Meets Programming - The Rise of Code Intelligence  https://arxiv.org/pdf/2401.14196  

## Qwen
- Qwen3: https://github.com/QwenLM/Qwen3/blob/main/Qwen3_Technical_Report.pdf  

## Gemini
- Gemini 2.5: https://link.zhihu.com/?target=https%3A//storage.googleapis.com/deepmind-media/gemini/gemini_v2_5_report.pdf

## Kimi
- K2: https://github.com/MoonshotAI/Kimi-K2/blob/main/tech_report.pdf  

## GLM
- GLM-4.5: https://arxiv.org/abs/2508.06471

## o1
- let's verify step by step: https://arxiv.org/pdf/2305.20050
- Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters: https://arxiv.org/pdf/2408.03314
  
## 其它
- ChatGLM2
[CODE](https://github.com/THUDM/ChatGLM2-6B)
- LLaMA: Open and Efficient Foundation Language Models
[PDF](https://arxiv.org/pdf/2302.13971)
[CODE](https://github.com/facebookresearch/llama)
- ChatGLM
[CODE](https://github.com/THUDM/ChatGLM-6B)
- PaLM: Scaling Language Modeling with Pathways
[PDF](https://arxiv.org/pdf/2204.02311.pdf)
- InstructGPT
[PDF](https://arxiv.org/pdf/2203.02155)
- GPT 3.0
[PDF](https://papers.nips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf)
- T5: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer
[PDF](https://arxiv.org/pdf/1910.10683.pdf)
[CODE](https://github.com/google-research/text-to-text-transfer-transformer)
- GPT 2.0
[PDF](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
- GPT 1.0
[PDF](https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf)
- BERT
[PDF](https://aclanthology.org/N19-1423.pdf)
- Transformers
[PDF](https://arxiv.org/pdf/1706.03762.pdf)

# Transformer原理
- transformer-explainer  https://poloclub.github.io/transformer-explainer
- The Illustrated Transformer  http://jalammar.github.io/illustrated-transformer/
- 
# 工具
- [langchain](https://github.com/hwchase17/langchain)

# 书籍
- [大语言模型](https://llmbook-zh.github.io/)

# 评测
- [A Survey on Evaluation of Large Language Models](https://arxiv.org/abs/2307.03109)
