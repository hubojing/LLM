# LLM
大模型自用资料。

# 论文 & 模型
- DeepSeek-R1: Incentivizing Reasoning Capability in LLMs viaReinforcement Learning  https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf
- DeepSeek-V3 Technical Report  https://github.com/deepseek-ai/DeepSeek-V3/blob/main/DeepSeek_V3.pdf
- DeepSeek-Coder: When the Large Language Model Meets Programming - The Rise of Code Intelligence  https://arxiv.org/pdf/2401.14196

- ChatGLM2
[CODE](https://github.com/THUDM/ChatGLM2-6B)
- LLaMA: Open and Efficient Foundation Language Models
[PDF](https://arxiv.org/pdf/2302.13971)
[CODE](https://github.com/facebookresearch/llama)
- ChatGLM
[CODE](https://github.com/THUDM/ChatGLM-6B)
- PaLM: Scaling Language Modeling with Pathways
[PDF](https://arxiv.org/pdf/2204.02311.pdf)
- InstructGPT
[PDF](https://arxiv.org/pdf/2203.02155)
- GPT 3.0
[PDF](https://papers.nips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf)
- T5: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer
[PDF](https://arxiv.org/pdf/1910.10683.pdf)
[CODE](https://github.com/google-research/text-to-text-transfer-transformer)
- GPT 2.0
[PDF](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
- GPT 1.0
[PDF](https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf)
- BERT
[PDF](https://aclanthology.org/N19-1423.pdf)
- Transformers
[PDF](https://arxiv.org/pdf/1706.03762.pdf)

# Transformer原理
- transformer-explainer  https://poloclub.github.io/transformer-explainer
- The Illustrated Transformer  http://jalammar.github.io/illustrated-transformer/
- 
# 工具
- [langchain](https://github.com/hwchase17/langchain)

# 书籍
- [大预言模型](https://llmbook-zh.github.io/)

# 评测
- [A Survey on Evaluation of Large Language Models](https://arxiv.org/abs/2307.03109)
